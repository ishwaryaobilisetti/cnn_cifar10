{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CIFAR-10 Image Classification with CNN\n",
                "\n",
                "## Objective\n",
                "Build and train a Convolutional Neural Network (CNN) from scratch using TensorFlow and Keras to classify images from the CIFAR-10 dataset.\n",
                "This project demonstrates data loading, preprocessing, model architecture design, data augmentation, training with callbacks, and comprehensive evaluation.\n",
                "\n",
                "## Requirements\n",
                "- Setup Python environment with TensorFlow, NumPy, Matplotlib, Seaborn, Scikit-learn.\n",
                "- Achieve >75% accuracy on test set.\n",
                "- Implement Data Augmentation and Learning Rate Scheduling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential, load_model, Model\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "tf.random.set_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Preprocessing\n",
                "We load the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 classes.\n",
                "The pixel values are normalized to the range [0, 1]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load CIFAR-10 dataset\n",
                "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
                "\n",
                "# Define class names\n",
                "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
                "\n",
                "# Normalize pixel values to be between 0 and 1\n",
                "x_train = x_train.astype('float32') / 255.0\n",
                "x_test = x_test.astype('float32') / 255.0\n",
                "\n",
                "# One-hot encode the labels\n",
                "num_classes = 10\n",
                "y_train_cat = to_categorical(y_train, num_classes)\n",
                "y_test_cat = to_categorical(y_test, num_classes)\n",
                "\n",
                "print(f\"Training data shape: {x_train.shape}\")\n",
                "print(f\"Test data shape: {x_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exploratory Data Analysis (EDA)\n",
                "Visualizing sample images from the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 10))\n",
                "for i in range(25):\n",
                "    plt.subplot(5, 5, i+1)\n",
                "    plt.xticks([])\n",
                "    plt.yticks([])\n",
                "    plt.grid(False)\n",
                "    plt.imshow(x_train[i])\n",
                "    # The CIFAR labels happen to be arrays, hence the extra index\n",
                "    plt.xlabel(class_names[y_train[i][0]])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. CNN Architecture\n",
                "We design a Sequential CNN model with:\n",
                "- 3 Convolutional blocks (Conv2D + ReLU + MaxPool + Dropout)\n",
                "- Flatten layer\n",
                "- Dense layers with Dropout\n",
                "- Final Softmax layer for 10 classes\n",
                "\n",
                "This architecture is designed to capture hierarchical features while preventing overfitting using Dropout."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_model():\n",
                "    model = Sequential([\n",
                "        Input(shape=(32, 32, 3)),\n",
                "        \n",
                "        # Block 1\n",
                "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D((2, 2)),\n",
                "        Dropout(0.2),\n",
                "        \n",
                "        # Block 2\n",
                "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D((2, 2)),\n",
                "        Dropout(0.3),\n",
                "        \n",
                "        # Block 3\n",
                "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D((2, 2)),\n",
                "        Dropout(0.4),\n",
                "        \n",
                "        Flatten(),\n",
                "        Dense(128, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.5),\n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ])\n",
                "    return model\n",
                "\n",
                "model = create_model()\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Augmentation\n",
                "To improve generalization, we use `ImageDataGenerator` to augment the training data with random horizontal flips, rotations, and shifts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datagen = ImageDataGenerator(\n",
                "    rotation_range=15,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1,\n",
                "    horizontal_flip=True,\n",
                "    zoom_range=0.1\n",
                ")\n",
                "datagen.fit(x_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training\n",
                "We compile the model with Adam optimizer and Categorical Crossentropy loss.\n",
                "We use callbacks:\n",
                "- `ReduceLROnPlateau`: To lower learning rate if validation loss plateaus.\n",
                "- `ModelCheckpoint`: To save the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.compile(optimizer='adam',\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
                "checkpoint = ModelCheckpoint('cifar10_best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
                "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
                "\n",
                "# Check for GPU availability\n",
                "physical_devices = tf.config.list_physical_devices('GPU')\n",
                "if len(physical_devices) > 0:\n",
                "    print(\"GPU detected. Setting epochs to 50 for full training.\")\n",
                "    epochs = 50\n",
                "else:\n",
                "    print(\"No GPU detected. Training on CPU is very slow.\")\n",
                "    print(\"Setting epochs to 5 for demonstration purposes. Accuracy will be low.\")\n",
                "    print(\"To achieve >75% accuracy, please run this on a machine with a GPU (e.g., Google Colab) for 50 epochs.\")\n",
                "    epochs = 5\n",
                "\n",
                "batch_size = 64\n",
                "\n",
                "history = model.fit(datagen.flow(x_train, y_train_cat, batch_size=batch_size),\n",
                "                    epochs=epochs,\n",
                "                    validation_data=(x_test, y_test_cat),\n",
                "                    callbacks=[reduce_lr, checkpoint, early_stop],\n",
                "                    verbose=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation\n",
                "We evaluate the model on the test set and visualize the performance.\n",
                "Note: If you are running this notebook, the following cells depend on the trained model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model if available, else use current\n",
                "try:\n",
                "    best_model = load_model('cifar10_best_model.keras')\n",
                "    print(\"Loaded best model from disk\")\n",
                "except:\n",
                "    print(\"Using current model (checkpoints not found)\")\n",
                "    best_model = model\n",
                "\n",
                "loss, acc = best_model.evaluate(x_test, y_test_cat, verbose=0)\n",
                "print(f\"Test Accuracy: {acc*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
                "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
                "plt.title('Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['loss'], label='Training Loss')\n",
                "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
                "plt.title('Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Classification Report and Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = best_model.predict(x_test)\n",
                "y_pred_classes = np.argmax(y_pred, axis=1)\n",
                "y_true = np.argmax(y_test_cat, axis=1)\n",
                "\n",
                "print(\"Classification Report:\\n\")\n",
                "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred_classes)\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('True')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Misclassified Images\n",
                "Visualizing 10 images where the model made errors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "misclassified_idx = np.where(y_pred_classes != y_true)[0]\n",
                "plt.figure(figsize=(15, 6))\n",
                "if len(misclassified_idx) > 0:\n",
                "    for i in range(min(10, len(misclassified_idx))):\n",
                "        idx = misclassified_idx[i]\n",
                "        plt.subplot(2, 5, i+1)\n",
                "        plt.imshow(x_test[idx])\n",
                "        plt.title(f\"True: {class_names[y_true[idx]]}\\nPred: {class_names[y_pred_classes[idx]]}\", color='red')\n",
                "        plt.axis('off')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Feature Maps Visualization\n",
                "Visualizing the output of the first convolutional layer for a sample input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "17a705fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract output of the first Conv2D layer\n",
                "import numpy as np\n",
                "from tensorflow.keras.models import Model\n",
                "\n",
                "# Ensure model is built (Keras 3 fix)\n",
                "try:\n",
                "    best_model.predict(np.zeros((1, 32, 32, 3)), verbose=0)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "layer_outputs = [layer.output for layer in best_model.layers if 'conv2d' in layer.name]\n",
                "activation_model = Model(inputs=best_model.inputs, outputs=layer_outputs)\n",
                "\n",
                "sample_image = x_test[0].reshape(1, 32, 32, 3)\n",
                "activations = activation_model.predict(sample_image)\n",
                "\n",
                "first_layer_activation = activations[0]\n",
                "print(f\"First Conv2D Activation Shape: {first_layer_activation.shape}\")\n",
                "\n",
                "plt.figure(figsize=(16, 8))\n",
                "num_filters = first_layer_activation.shape[-1]\n",
                "cols = 8\n",
                "rows = num_filters // cols\n",
                "\n",
                "for i in range(min(32, num_filters)):\n",
                "    plt.subplot(rows if rows > 0 else 1, cols, i+1)\n",
                "    plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
                "    plt.axis('off')\n",
                "plt.suptitle('Feature Maps (First Conv2D Layer)')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
